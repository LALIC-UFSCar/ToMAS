{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5c88f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import emoji\n",
    "import swifter\n",
    "import warnings\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from numba import cuda\n",
    "from llama_cpp import Llama\n",
    "from bertopic import BERTopic\n",
    "from cuml.manifold import UMAP\n",
    "from timeout_decorator import timeout\n",
    "from cuml.cluster import HDBSCAN, DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from cuml.metrics.cluster import silhouette_score as silhouette_cuml\n",
    "\n",
    "#Importações para o Llama 2\n",
    "from huggingface_hub import hf_hub_download\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "##%load_ext cudf.pandas\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d24411",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Suprimir todos os warnings do Pandas\n",
    "warnings.filterwarnings('ignore', category=pd.errors.DtypeWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"No sentence-transformers model found\")\n",
    "\n",
    "# Restaurar as configurações de aviso padrão (opcional)\n",
    "warnings.filterwarnings('default')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14becd74",
   "metadata": {},
   "source": [
    "## Funções de pré-processamento dos textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6fb598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para remover menções a usuários (@algumusuario)\n",
    "def remove_mentions(text):\n",
    "    return re.sub(r'@[\\w_]+', '', text)\n",
    "\n",
    "\n",
    "# Função para remover acentos repetidos (!!!, ???, etc)\n",
    "def remove_repeated_punctuation(text):\n",
    "    return re.sub(r'([!?.])\\1+', r'\\1', text)\n",
    "\n",
    "\n",
    "# Função para remover emojis\n",
    "def remove_emojis(data):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\"\n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', data).strip()\n",
    "\n",
    "\n",
    "# Função para capitalizar as palavras (início de frase maisculo, restante minusculo)\n",
    "def capitalize_text(text):\n",
    "    #Strip\n",
    "    text = text.strip()\n",
    "    # Dividir o texto em sentenças\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    # Capitalizar a primeira letra de cada sentença\n",
    "    capitalized_sentences = [sentence.capitalize() for sentence in sentences]\n",
    "    # Juntar as sentenças novamente em um único texto\n",
    "    return ' '.join(capitalized_sentences)\n",
    "\n",
    "\n",
    "# Função para remover hiperlinks.\n",
    "def remove_links_and_line_breaks(text):\n",
    "    # Remove hiperlinks\n",
    "    text = re.sub(r'http\\S+|www.\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Substitui quebras de linha por espaços\n",
    "    text = re.sub(r'\\n\\d*', ' ', text)\n",
    "    # Substitui quebras de linha no meio de palavras por um espaço\n",
    "    text = re.sub(r'(\\w+)\\n(\\w+)', r'\\1 \\2', text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "# Função para remover caracteres especiais e substituir emojis pelas suas palavras.\n",
    "def replace_special_characters(text):\n",
    "    # Converte emojis de números em seus valores numéricos\n",
    "    text = emoji.demojize(text)\n",
    "    # Define um dicionário com a correspondência entre emojis e números\n",
    "    emoji_to_text = {\n",
    "        \":keycap_0:\": \"0\",\n",
    "        \":keycap_1:\": \"1\",\n",
    "        \":keycap_2:\": \"2\",\n",
    "        \":keycap_3:\": \"3\",\n",
    "        \":keycap_4:\": \"4\",\n",
    "        \":keycap_5:\": \"5\",\n",
    "        \":keycap_6:\": \"6\",\n",
    "        \":keycap_7:\": \"7\",\n",
    "        \":keycap_8:\": \"8\",\n",
    "        \":keycap_9:\": \"9\",\n",
    "        \":squid:\":\"Lula\",\n",
    "        \":brazil:\":\"Brasil\"\n",
    "    }\n",
    "    # Substitui os emojis de números pelos valores numéricos\n",
    "    for emoji_text, number in emoji_to_text.items():\n",
    "        text = text.replace(emoji_text, number)\n",
    "    return emoji.emojize(text)\n",
    "\n",
    "\n",
    "# Função para remover hashtags.\n",
    "def remove_hashtags(text):\n",
    "    # Use a expressão regular para encontrar e remover hashtags\n",
    "    text_without_hashtags = re.sub(r'#\\w+', '', text)\n",
    "    return text_without_hashtags.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fe9207-7ef2-42d1-bc7b-d26e5aa0c84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Obtenção dos tweets de 8 a 15 de jan de 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388453e2-e217-4ab8-ad9b-41c249132c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "ano = '2023'\n",
    "mes = '01'\n",
    "tipo = 'tweets'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6c56a5-6c8b-48de-9a1e-28e97e3cd45b",
   "metadata": {},
   "source": [
    "## Leitura Tweets Bolsonaro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187eb818-87ed-4a51-a818-77fc5544474a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'query_bolsonaro'\n",
    "data_list = []\n",
    "\n",
    "for dia in range(8, 13):\n",
    "    dia_str = f\"{dia:02d}\"\n",
    "    diretorio = f'./corpus/{query}/{tipo}-{query}-{ano}_{mes}_{dia_str}.parquet'\n",
    "\n",
    "    print(diretorio)\n",
    "    # Tenta ler o arquivo parquet, se existir\n",
    "    try:\n",
    "        data = pd.read_parquet(diretorio)\n",
    "        data_list.append(data)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Arquivo não encontrado para o dia {dia}\")\n",
    "\n",
    "df_bolsonaro = pd.concat(data_list, ignore_index=True)\n",
    "df_bolsonaro = df_bolsonaro[['id', 'text', 'created_at', 'hashtags']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeda43db-5115-4a08-a495-c237ba7f5652",
   "metadata": {},
   "source": [
    "## Leitura Tweets Lula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47aad70-5b5c-4f71-98f1-7218bfcef027",
   "metadata": {},
   "outputs": [],
   "source": [
    "query='query_lula'\n",
    "data_list = []\n",
    "\n",
    "for dia in range(8, 13):\n",
    "    dia_str = f\"{dia:02d}\"\n",
    "    diretorio = f'./corpus/{query}/{tipo}-{query}-{ano}_{mes}_{dia_str}.parquet'\n",
    "\n",
    "    print(diretorio)\n",
    "    # Tenta ler o arquivo parquet, se existir\n",
    "    try:\n",
    "        data = pd.read_parquet(diretorio)\n",
    "        data_list.append(data)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Arquivo não encontrado para o dia {dia}\")\n",
    "\n",
    "df_lula = pd.concat(data_list, ignore_index=True)\n",
    "df_lula = df_lula[['id', 'text', 'created_at', 'hashtags']][:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec39223c-c77b-48bf-bd09-ac35739e9e41",
   "metadata": {},
   "source": [
    "## Leitura Tweets Ato Golpista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82af936c-a344-490f-a34e-5ae5c3f345b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query='atos_golpistas'\n",
    "data_list = []\n",
    "\n",
    "for dia in range(8, 11):\n",
    "    dia_str = f\"{dia:02d}\"\n",
    "    diretorio = f'./corpus/dados/{tipo}-{query}/{query}/{tipo}-{query}-{ano}_{mes}_{dia_str}.parquet'\n",
    "\n",
    "    print(diretorio)\n",
    "    # Tenta ler o arquivo parquet, se existir\n",
    "    try:\n",
    "        data = pd.read_parquet(diretorio)\n",
    "        data_list.append(data)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Arquivo não encontrado para o dia {dia}\")\n",
    "\n",
    "df_atos_golpistas = pd.concat(data_list, ignore_index=True)\n",
    "df_atos_golpistas = df_atos_golpistas[['id', 'text', 'created_at', 'hashtags']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2692f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar as funções aos dados da coluna \"text\"\n",
    "\n",
    "def tratamento_base(df):\n",
    "    df['text'] = df['text'].apply(remove_mentions)\n",
    "    df['text'] = df['text'].apply(remove_repeated_punctuation)\n",
    "    df['text'] = df['text'].apply(remove_links_and_line_breaks)\n",
    "    df['text'] = df['text'].apply(replace_special_characters)\n",
    "    df['text'] = df['text'].apply(remove_emojis)\n",
    "    df['text'] = df['text'].apply(remove_hashtags)\n",
    "    df['text'] = df['text'].apply(capitalize_text)\n",
    "    \n",
    "    # Remover linhas com valores em branco na coluna \"text\"\n",
    "    df = df[df['text'].notna() & df['text'] != '']\n",
    "    \n",
    "    # Remover linhas duplicadas com base na coluna \"text\"\n",
    "    df = df.drop_duplicates(subset='text')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fedfe32-73f6-4ede-8c29-17201db38c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tratamento_base(df):\n",
    "    df['text'] = df['text'].swifter.apply(remove_mentions)\n",
    "    df['text'] = df['text'].swifter.apply(remove_repeated_punctuation)\n",
    "    df['text'] = df['text'].swifter.apply(remove_links_and_line_breaks)\n",
    "    df['text'] = df['text'].swifter.apply(replace_special_characters)\n",
    "    df['text'] = df['text'].swifter.apply(remove_emojis)\n",
    "    df['text'] = df['text'].swifter.apply(remove_hashtags)\n",
    "    df['text'] = df['text'].swifter.apply(capitalize_text)\n",
    "    # Remover linhas com valores em branco na coluna \"text\"\n",
    "    df = df[df['text'].notna() & df['text'] != '']\n",
    "    # Remover linhas duplicadas com base na coluna \"text\"\n",
    "    df = df.drop_duplicates(subset='text')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cb57c9-5ade-4123-be04-d1fd89fe35b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bolsonaro = tratamento_base(df_bolsonaro)\n",
    "df_lula = tratamento_base(df_lula)\n",
    "df_atos_golpistas = tratamento_base(df_atos_golpistas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bba0687-a0c2-4ae6-9530-b6f59f12d49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_atos_golpistas = tratamento_base(df_atos_golpistas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa7cf0c",
   "metadata": {},
   "source": [
    "## Geração das embeddings das sentenças"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1bd8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a5fd1d-d31f-46de-95bc-b2d7fcf3822e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings_generation(df):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    #Seleção do modelo bertimbau para a geração das embeddings\n",
    "    model = SentenceTransformer('neuralmind/bert-large-portuguese-cased', device = device)\n",
    "    \n",
    "    sentences = df['text']\n",
    "    sentences = sentences.tolist()\n",
    "\n",
    "    # Geração de embeddings de texto\n",
    "    embeddings = model.encode(sentences, show_progress_bar=True, device=device)\n",
    "\n",
    "    del(model)\n",
    "    \n",
    "    #Normalização das embeddings\n",
    "    normalized_embeddings = embeddings / np.linalg.norm(embeddings, axis=1)[:, np.newaxis]\n",
    "\n",
    "    # Redução de dimensionalidade com UMAP\n",
    "    umap_embeddings = UMAP(n_neighbors=10,  # Reduzido para 10 vizinhos\n",
    "                                n_components=512,\n",
    "                                metric='cosine').fit_transform(normalized_embeddings)\n",
    "    \n",
    "    #df['embedding_default'] = embeddings.tolist()\n",
    "    df['embedding_normalized'] = normalized_embeddings.tolist()\n",
    "    df['embedding'] = umap_embeddings.tolist()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4b4768",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bolsonaro = embeddings_generation(df_bolsonaro)\n",
    "df_bolsonaro.to_parquet('df_bolsonaro_processed.parquet', index=False)\n",
    "del(df_bolsonaro)\n",
    "\n",
    "df_lula = embeddings_generation(df_lula)\n",
    "df_lula.to_parquet('df_lula_processed.parquet', index=False)\n",
    "del(df_lula)\n",
    "\n",
    "df_atos_golpistas = embeddings_generation(df_atos_golpistas)\n",
    "df_atos_golpistas.to_parquet('df_atos_golpistas_processed.parquet', index=False)\n",
    "del(df_atos_golpistas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6ee0c8-32a1-4a60-ba04-75a793db8523",
   "metadata": {},
   "source": [
    "## Obtenção das melhores métricas do HDBSCan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301bc93c-7340-478c-8cfd-b0f279f4df96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def get_cluster_results_metrics(df):\n",
    "    # Lista para armazenar os resultados\n",
    "    resultados = []\n",
    "\n",
    "    embeddings = np.array(df['embedding'].tolist())\n",
    "    \n",
    "    # Iterando sobre os parâmetros\n",
    "    for min_cluster_size in range(10, 201, 10):\n",
    "        for metric in ['euclidean']:\n",
    "            for cluster_selection_method in ['leaf', 'eom']:\n",
    "                print(\"Início: \", datetime.datetime.now())\n",
    "                # Clustering com HDBSCAN\n",
    "                cluster = HDBSCAN(min_cluster_size=min_cluster_size,\n",
    "                                  metric=metric,\n",
    "                                  cluster_selection_method=cluster_selection_method).fit(embeddings)\n",
    "    \n",
    "                # Verificar se há pelo menos dois rótulos únicos\n",
    "                unique_labels = np.unique(cluster.labels_)\n",
    "                print(\"Gerou o cluster: \", datetime.datetime.now())\n",
    "                if len(unique_labels) > 1:\n",
    "                    # Calcular as métricas de avaliação\n",
    "                    labels = cluster.labels_\n",
    "\n",
    "                    cluster=None\n",
    "                    del(cluster)\n",
    "                    \n",
    "                    try:\n",
    "                        print(\"Silhouette CUML\")\n",
    "                        silhouette_avg = silhouette_cuml(X=embeddings, labels = labels, metric='cosine')\n",
    "                    except Exception:\n",
    "                        print(\"Silhouette CPU\")\n",
    "                        silhouette_avg = silhouette_score(X=embeddings, labels = labels, metric='cosine')\n",
    "\n",
    "                    # Armazenar os resultados\n",
    "                    resultados.append({\n",
    "                        'min_cluster_size': min_cluster_size,\n",
    "                        'metric': metric,\n",
    "                        'cluster_selection_method': cluster_selection_method,\n",
    "                        'silhouette_score': silhouette_avg,\n",
    "                        'labels': len(unique_labels)\n",
    "                    })\n",
    "    \n",
    "                    # Imprimir os resultados de cada rodada\n",
    "                    print(f\"Parâmetros: min_cluster_size={min_cluster_size}, metric={metric}, cluster_selection_method={cluster_selection_method}\")\n",
    "                    print(f\"Silhueta: {silhouette_avg}\")\n",
    "                    print(f\"Labels: {len(unique_labels)}\")\n",
    "\n",
    "                    print(\"Fim: \", datetime.datetime.now())\n",
    "                    print(\"-\" * 30)\n",
    "\n",
    "    return resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf823076-001c-4d0e-8ae9-c0cd86c32e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering(df, melhor_resultado):\n",
    "\n",
    "    print(\"Início: \", datetime.datetime.now())\n",
    "    \n",
    "    embeddings = np.array(df['embedding'].tolist())\n",
    "    \n",
    "    # Clustering com HDBSCAN - Pulando a redução de dimensionalidade por enquanto\n",
    "    cluster = HDBSCAN(min_cluster_size=melhor_resultado['min_cluster_size'],      # Aumentado para 20\n",
    "                          metric=melhor_resultado['metric'],       # Alterado para distância euclideana\n",
    "                          cluster_selection_method=melhor_resultado['cluster_selection_method']).fit(embeddings)\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    # Preparação dos dados\n",
    "    umap_data = UMAP(n_neighbors=10, n_components=2, metric='cosine').fit_transform(embeddings)\n",
    "    result = pd.DataFrame(umap_data, columns=['x', 'y'])\n",
    "    result['labels'] = cluster.labels_\n",
    "    \n",
    "    # Visualização dos clusters\n",
    "    fig, ax = plt.subplots(figsize=(20, 10))\n",
    "    outliers = result.loc[result.labels == -1, :]\n",
    "    clustered = result.loc[result.labels != -1, :]\n",
    "    plt.scatter(outliers.x, outliers.y, color='#BDBDBD', s=0.05)\n",
    "    plt.scatter(clustered.x, clustered.y, c=clustered.labels, s=0.05, cmap='hsv_r')\n",
    "    plt.colorbar()\n",
    "\n",
    "    df['label'] = cluster.labels_\n",
    "\n",
    "    print(\"Fim: \", datetime.datetime.now())\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fda206-d18c-4941-b4b1-ab4e6b6ebc60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_bolsonaro_processed = pd.read_parquet('df_bolsonaro_processed.parquet')\n",
    "resultados_bolsonaro = get_cluster_results_metrics(df_bolsonaro_processed)\n",
    "\n",
    "with open('./results/clustering/resultados_cluster_bolsonaro.txt', 'w') as arquivo:\n",
    "    arquivo.write('\\n'.join(map(str, resultados_bolsonaro)))\n",
    "\n",
    "melhor_resultado_bolsonaro = max(resultados_bolsonaro, key=lambda x: x['silhouette_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ce1578-6613-49a4-8e95-b466243fab71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lula_processed = pd.read_parquet('df_lula_processed.parquet').iloc[:500000]\n",
    "resultados_lula = get_cluster_results_metrics(df_lula_processed)\n",
    "\n",
    "with open('./results/clustering/resultados_cluster_lula.txt', 'w') as arquivo:\n",
    "    arquivo.write('\\n'.join(map(str, resultados_lula)))\n",
    "\n",
    "melhor_resultado_lula = max(resultados_lula, key=lambda x: x['silhouette_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c79cb7-5dc0-4a30-8372-832489dca14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_atos_golpistas_processed = pd.read_parquet('df_atos_golpistas_processed.parquet').iloc[:500000]\n",
    "resultados_atos_golpistas = get_cluster_results_metrics(df_atos_golpistas_processed)\n",
    "\n",
    "with open('./results/clustering/resultados_cluster_atos_golpistas.txt', 'w') as arquivo:\n",
    "    arquivo.write('\\n'.join(map(str, resultados_atos_golpistas)))\n",
    "\n",
    "melhor_resultado_atos_golpistas = max(resultados_atos_golpistas, key=lambda x: x['silhouette_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec21e4df-4dd2-4c4b-84a5-107ff13318ea",
   "metadata": {},
   "source": [
    "## Extração de Tópicos - BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a094125f-ce97-4f78-9219-223323f1349e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_topics_bertopic(df):\n",
    "    embeddings = np.array(df['embedding_normalized'].tolist())\n",
    "    docs = df['text'].tolist()\n",
    "\n",
    "    hdbscan_model = HDBSCAN(min_samples=10, gen_min_span_tree=True, cluster_selection_method='eom', prediction_data=True)\n",
    "    umap_model = UMAP(n_neighbors=15, n_components=512, min_dist=0.0, metric='cosine', random_state=42)\n",
    "\n",
    "    topic_model = BERTopic(hdbscan_model=hdbscan_model, umap_model=umap_model)\n",
    "\n",
    "    topics, probs = topic_model.fit_transform(docs, embeddings)\n",
    "\n",
    "    topic_info = topic_model.get_topic_info()\n",
    "\n",
    "    del hdbscan_model, umap_model, topic_model\n",
    "\n",
    "    return topics, probs, topic_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1767da-3ad8-473d-9b01-c46148ba4857",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bolsonaro_processed = pd.read_parquet('df_bolsonaro_processed.parquet')\n",
    "topics, probs, topic_info = generate_topics_bertopic(df_bolsonaro_processed)\n",
    "\n",
    "df_bolsonaro_processed['topics'] = topics\n",
    "\n",
    "df_bolsonaro_processed.to_parquet('df_bolsonaro_topic.parquet', index=False)\n",
    "topic_info.to_parquet('topicos_bolsonaro.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd405430-626d-4d4f-a845-030782b2d263",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lula_processed = pd.read_parquet('df_lula_processed.parquet')\n",
    "topics, probs, topic_info = generate_topics_bertopic(df_lula_processed)\n",
    "\n",
    "df_lula_processed['topics'] = topics\n",
    "\n",
    "df_lula_processed.to_parquet('df_lula_topic.parquet', index=False)\n",
    "topic_info.to_parquet('topicos_lula.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262b1350-d3c3-475f-b486-4fa22fae6d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_atos_golpistas_processed = pd.read_parquet('df_atos_golpistas_processed.parquet')\n",
    "topics, probs, topic_info = generate_topics_bertopic(df_atos_golpistas_processed)\n",
    "\n",
    "df_atos_golpistas_processed['topics'] = topics\n",
    "\n",
    "df_atos_golpistas_processed.to_parquet('df_atos_golpistas_topic.parquet', index=False)\n",
    "topic_info.to_parquet('topicos_atos_golpistas.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718b447c-7fce-45ba-9611-4b9f2db5717c",
   "metadata": {},
   "source": [
    "## Análise descritiva simples dos dados finais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9be4d80-f91c-4c68-b611-80559e4efa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bolsonaro_topic = pd.read_parquet('df_bolsonaro_topic.parquet')\n",
    "df_bolsonaro_topic.drop(df_bolsonaro_topic[df_bolsonaro_topic.topics == -1].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43e3842-4337-4686-a322-dc5e7d97e454",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lula_topic = pd.read_parquet('df_lula_topic.parquet')\n",
    "df_lula_topic.drop(df_lula_topic[df_lula_topic.topics == -1].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e699d6e3-e71a-4d3e-b033-fc2e5aef872d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_atos_golpistas_topic = pd.read_parquet('df_atos_golpistas_topic.parquet')\n",
    "df_atos_golpistas_topic.drop(df_atos_golpistas_topic[df_atos_golpistas_topic.topics == -1].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1369e37e-77b4-484f-ab8d-92d4d4f81003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculando as estatísticas\n",
    "topic_counts = df_bolsonaro_topic['topics'].value_counts()\n",
    "max_topic = topic_counts.idxmax()\n",
    "min_topic = topic_counts.idxmin()\n",
    "mean_topic = topic_counts.mean()\n",
    "median_topic = topic_counts.median()\n",
    "\n",
    "summary = {\n",
    "    \"Topic with Max Entries\": max_topic,\n",
    "    \"Max Entries\": topic_counts[max_topic],\n",
    "    \"Topic with Min Entries\": min_topic,\n",
    "    \"Min Entries\": topic_counts[min_topic],\n",
    "    \"Mean of Entries\": mean_topic,\n",
    "    \"Median of Entries\": median_topic\n",
    "}\n",
    "\n",
    "# Exibindo o resumo\n",
    "summary_df = pd.DataFrame([summary])\n",
    "summary_df\n",
    "\n",
    "# 925 tópicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600641eb-5202-4470-8581-437c2dff3483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculando as estatísticas\n",
    "topic_counts = df_lula_topic['topics'].value_counts()\n",
    "max_topic = topic_counts.idxmax()\n",
    "min_topic = topic_counts.idxmin()\n",
    "mean_topic = topic_counts.mean()\n",
    "median_topic = topic_counts.median()\n",
    "\n",
    "summary = {\n",
    "    \"Topic with Max Entries\": max_topic,\n",
    "    \"Max Entries\": topic_counts[max_topic],\n",
    "    \"Topic with Min Entries\": min_topic,\n",
    "    \"Min Entries\": topic_counts[min_topic],\n",
    "    \"Mean of Entries\": mean_topic,\n",
    "    \"Median of Entries\": median_topic\n",
    "}\n",
    "\n",
    "# Exibindo o resumo\n",
    "summary_df = pd.DataFrame([summary])\n",
    "summary_df\n",
    "\n",
    "# 1306 tópicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47571e95-dd58-4a97-aa3c-599a0ec45cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculando as estatísticas\n",
    "topic_counts = df_atos_golpistas_topic['topics'].value_counts()\n",
    "max_topic = topic_counts.idxmax()\n",
    "min_topic = topic_counts.idxmin()\n",
    "mean_topic = topic_counts.mean()\n",
    "median_topic = topic_counts.median()\n",
    "\n",
    "summary = {\n",
    "    \"Topic with Max Entries\": max_topic,\n",
    "    \"Max Entries\": topic_counts[max_topic],\n",
    "    \"Topic with Min Entries\": min_topic,\n",
    "    \"Min Entries\": topic_counts[min_topic],\n",
    "    \"Mean of Entries\": mean_topic,\n",
    "    \"Median of Entries\": median_topic\n",
    "}\n",
    "\n",
    "# Exibindo o resumo\n",
    "summary_df = pd.DataFrame([summary])\n",
    "summary_df\n",
    "\n",
    "# 1646 tópicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48ab642",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_top_twenty_topics(df):\n",
    "    top_ten = df['topics'].value_counts().nlargest(20).to_dict()\n",
    "    \n",
    "    topten_list = []\n",
    "    for key, value in top_ten.items():\n",
    "        topten_list.append(key)\n",
    "        \n",
    "    return df[df['topics'].isin(topten_list)].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d69f9a-2449-40ae-854c-5afc5c769f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_top_twenty_topics_df_atos_golpistas = filter_top_twenty_topics(df_atos_golpistas_topic)\n",
    "filter_top_twenty_topics_df_atos_golpistas.to_csv('filter_top_twenty_topics_df_atod_golpistas.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b7c040-c881-4611-a17e-32828721cdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_top_twenty_topics_df_lula = filter_top_twenty_topics(df_lula_topic)\n",
    "filter_top_twenty_topics_df_lula.to_csv('filter_top_twenty_topics_df_lula.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605236b5-3d29-485c-8115-633ca5a70022",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_top_twenty_topics_df_bolsonaro = filter_top_twenty_topics(df_bolsonaro_topic)\n",
    "filter_top_twenty_topics_df_bolsonaro.to_csv('filter_top_twenty_topics_df_bolsonaro.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd3c762",
   "metadata": {},
   "source": [
    "## Sumarização dos tweets usando Llama 2 - 13B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb64891-5a7e-4e38-b7c0-fc45a9b7667a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download do modelo\n",
    "def download_hf_model(repo_id, filename):\n",
    "    downloaded_model_path = hf_hub_download(\n",
    "        repo_id=repo_id,\n",
    "        filename=filename,\n",
    "        use_auth_token=True)\n",
    "\n",
    "    return downloaded_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396c3572-bd36-445a-9397-4adc690db74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função do llama com prompt para a sumarização\n",
    "@timeout(75)\n",
    "def llama_summarizer(llm, tweets, model):\n",
    "    tweets = \";\".join(tweets).replace('\\n', '')\n",
    "\n",
    "    if model == 'llama':\n",
    "        template = \"\"\"[INST] <<SYS>> I am providing you with an 'Input' of a set of texts in Brazilian Portuguese that are separated by \";\" between them. Provide as 'Output' a summary in continuous text with your own words also in Brazilian Portuguese,\n",
    "        without repeating the original texts and without citing examples, covering the main subjects being mentioned in the input texts briefly and in a general way. Do not form your own opinions; all content in the summarization must be based on the \n",
    "        content found in the texts. When citing a trend or any behavior, use something like 'Os autores citaram que (...)' <<SYS>>\n",
    "    \n",
    "        Input: '''{tweets}''' [/INST]\n",
    "        \n",
    "        Output:\n",
    "        \"\"\"\n",
    "    elif model == 'bode':\n",
    "        template = \"\"\"[INST] <<SYS>> Estou fornecendo a você uma 'Entrada' de um conjunto de textos em português do Brasil que estão separados por \";\" entre eles. Forneça como 'Saída' um resumo em texto contínuo com suas próprias palavras também em português do Brasil,\n",
    "        sem repetir os textos originais e sem citar exemplos, abrangendo os principais temas mencionados nos textos de entrada de forma breve e geral. Não forme suas próprias opiniões; todo o conteúdo na sumarização deve ser baseado no\n",
    "        conteúdo encontrado nos textos. Ao citar uma tendência ou comportamento, use algo como 'Os autores citaram que (...).' <<SYS>>\n",
    "\n",
    "        Input: \"{tweets}\" [/INST]\n",
    "        \n",
    "        Output:\n",
    "        \"\"\"\n",
    "        \n",
    "    prompt = PromptTemplate(input_variables=[\"tweets\"], template=template)\n",
    "    runnable = prompt | llm | StrOutputParser()\n",
    "    response = runnable.invoke({\"tweets\": tweets})\n",
    "\n",
    "    response = response.replace(\"[]\", \"\")\n",
    "\n",
    "    return response\n",
    "\n",
    "@timeout(75)\n",
    "def llama_translate(llm, text):\n",
    "\n",
    "    template = \"\"\"[INST] <<SYS>> Traduza o texto abaixo para português do Brasil. Caso ele já esteja em Português, apenas repita ele como saída. <<SYS>>\n",
    "\n",
    "    Input: \"{text}\" [/INST]\n",
    "    \n",
    "    Output:\n",
    "        \"\"\"\n",
    "        \n",
    "    prompt = PromptTemplate(input_variables=[\"tweets\"], template=template)\n",
    "    runnable = prompt | llm | StrOutputParser()\n",
    "    response = runnable.invoke({\"text\": text})\n",
    "\n",
    "    response = response.replace(\"[]\", \"\")\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4602c478-66d0-48a7-901c-d7db5c4e49b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llama_llm(model_repo_path, model_filename):\n",
    "    ### Llama-CPP\n",
    "    callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "    n_gpu_layers = -1\n",
    "    n_batch = 768\n",
    "\n",
    "    model_path_hf = download_hf_model(model_repo_path, model_filename)\n",
    "\n",
    "    #Instanciação do Lllama\n",
    "    llm = LlamaCpp(\n",
    "        model_path=model_path_hf,\n",
    "        n_gpu_layers=n_gpu_layers,\n",
    "        n_batch=n_batch,\n",
    "        temperature = 0.3,\n",
    "        max_tokens = 768,\n",
    "        n_ctx = 8192,\n",
    "        top_p=1,\n",
    "        n_threads=8,\n",
    "        callback_manager=callback_manager,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8395d26-a221-4232-88aa-6a731cb30390",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def define_model(model_type):\n",
    "    if model_type=='llama':\n",
    "        llm = get_llama_llm(model_repo_path='TheBloke/Llama-2-13B-chat-GGUF', model_filename='llama-2-13b-chat.Q8_0.gguf')\n",
    "    elif model_type=='bode':\n",
    "        llm = get_llama_llm(model_repo_path='recogna-nlp/bode-13b-alpaca-pt-br-gguf', model_filename='bode-13b-alpaca-q8_0.gguf')\n",
    "    elif model_type=='mistral':\n",
    "        llm = get_llama_llm(model_repo_path='TheBloke/Mistral-7B-Instruct-v0.2-GGUF', model_filename='mistral-7b-instruct-v0.2.Q8_0.gguf')\n",
    "    \n",
    "    return llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4a4f80-ce73-41de-a5dc-7b2c6f7d7913",
   "metadata": {},
   "source": [
    "## Sumarização dos tweets usando o Llama 3 8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a4d74a-4c33-4f07-9dea-85a152c7cb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pipeline_llama3(model_id):\n",
    "    pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device=\"cuda\",\n",
    "    )\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe2a199-ba96-45f2-854d-6bae96a35268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama_3_summarizer(pipeline, tweets):\n",
    "    tweets = \";\".join(tweets).replace('\\n', '')\n",
    "\n",
    "    messages = [\n",
    "    {\"role\": \"system\", \"content\": \"\"\"I am providing you with an 'Input' of a set of texts in Brazilian Portuguese that are separated by \";\" between them. Provide as 'Output' a summary in continuous text with your own words also in Brazilian Portuguese, without repeating the original texts and without citing examples, covering the main subjects being mentioned in the input texts briefly and in a general way. Do not form your own opinions; all content in the summarization must be based on the content found in the texts. When citing a trend or any behavior, use something like 'Os autores citaram que (...)'\"\"\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Input: {tweets}\"},\n",
    "    ]\n",
    "\n",
    "    prompt = pipeline.tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    outputs = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=768,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.3,\n",
    "    top_p=1,\n",
    "    )\n",
    "\n",
    "    print(outputs[0][\"generated_text\"][len(prompt):])\n",
    "    return outputs[0][\"generated_text\"][len(prompt):]\n",
    "\n",
    "\n",
    "def llama_3_translate(pipeline, text):\n",
    "\n",
    "    messages = [\n",
    "    {\"role\": \"system\", \"content\": \"\"\"Traduza o texto dado como \"Input\" para português do Brasil. Caso ele já esteja em Português, apenas repita ele como saída.\"\"\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Input: {text}\"},\n",
    "    ]\n",
    "\n",
    "    prompt = pipeline.tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    outputs = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=768,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.3,\n",
    "    top_p=1,\n",
    "    )\n",
    "\n",
    "    return outputs[0][\"generated_text\"][len(prompt):]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb5bf1e-82fd-4aeb-8633-c0f5736f6113",
   "metadata": {},
   "source": [
    "## Pipeline de sumarização Multinível"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6545dcc-56a7-4f4a-b78b-00ef0be0aef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a função auxiliar para agrupar os tweets até atingir um limite de palavras\n",
    "def agrupar_tweets(tweets, limite_palavras=1536):\n",
    "    grupos = []\n",
    "    grupo_atual = []\n",
    "    palavras_no_grupo = 0\n",
    "    for tweet in tweets:\n",
    "        palavras_tweet = len(tweet.split())\n",
    "        if palavras_no_grupo + palavras_tweet <= limite_palavras:\n",
    "            grupo_atual.append(tweet)\n",
    "            palavras_no_grupo += palavras_tweet\n",
    "        else:\n",
    "            grupos.append(grupo_atual)\n",
    "            grupo_atual = [tweet]\n",
    "            palavras_no_grupo = palavras_tweet\n",
    "    if grupo_atual:\n",
    "        grupos.append(grupo_atual)\n",
    "    return grupos\n",
    "\n",
    "# Lógica principal para sumarizar os tweets de forma recursiva\n",
    "def sumarizar_tweets(tweets, model, llm=None, pipeline=None):\n",
    "    while len(tweets) > 1:\n",
    "        grupos = agrupar_tweets(tweets)\n",
    "        sumarios = []\n",
    "        for grupo in grupos:\n",
    "            try:\n",
    "                if model not in ('llama_3', 'llama_3.1'):\n",
    "                    sumario = llama_summarizer(llm=llm, tweets=grupo, model=model)\n",
    "                    sumarios.append(sumario)\n",
    "                else:\n",
    "                    sumario = llama_3_summarizer(pipeline=pipeline, tweets=grupo)\n",
    "                    sumarios.append(sumario)\n",
    "            except:\n",
    "                print(\"\\n\\nTimeout de execução no LLama. Continuando com os próximos grupos.\")\n",
    "                continue\n",
    "            \n",
    "        tweets = sumarios\n",
    "        print(\"Tamanho: \", len(tweets))\n",
    "    return tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e66a863-1456-4798-9a97-83efcac885f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summaries(df, model, llm=None, pipeline=None):\n",
    "    # Dicionário para armazenar os resultados\n",
    "    sumarios = {}\n",
    "    \n",
    "    # Itera sobre os diferentes tópicos\n",
    "    for topic in df['topics'].unique():\n",
    "        subset_df = df[df['topics'] == topic]\n",
    "        \n",
    "        # Converte a coluna \"text\" para lista\n",
    "        tweets = subset_df['text'].tolist()\n",
    "\n",
    "        # Aplica a sumarização\n",
    "        resultado = sumarizar_tweets(tweets, model, llm, pipeline)\n",
    "        \n",
    "        # Armazena o resultado no dicionário\n",
    "        sumarios[topic] = resultado\n",
    "\n",
    "    return sumarios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ca7db3-46d0-4610-beba-bd7b75dcd784",
   "metadata": {},
   "source": [
    "### Base atos golpistas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fdc004-675a-4e14-a55f-5aa2cb2d5147",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelos = ['llama', 'mistral', 'bode', 'llama_3', 'llama_3.1']\n",
    "\n",
    "for model in modelos:\n",
    "    pipeline = None\n",
    "    llm = None\n",
    "    \n",
    "    if model not in ('llama_3', 'llama_3.1'):\n",
    "        #Limpeza de VRAM\n",
    "        try:\n",
    "            del llm\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        llm = define_model(model)\n",
    "    else:\n",
    "        try:\n",
    "            del pipeline\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        except:\n",
    "            continue\n",
    "        if model == 'llama_3':\n",
    "            model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "        elif model == 'llama_3.1':\n",
    "            model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "        pipeline = get_pipeline_llama3(model_id)\n",
    "\n",
    "    sumarios_atos_golpistas = get_summaries(filter_top_twenty_topics_df_atos_golpistas, model, llm, pipeline)\n",
    "\n",
    "    for chave, mensagem in sumarios_atos_golpistas.items():\n",
    "        if model not in ('llama_3', 'llama_3.1'):\n",
    "            mensagem = llama_translate(llm, mensagem)\n",
    "        else:\n",
    "            mensagem = llama_3_translate(pipeline, mensagem)\n",
    "        sumarios_atos_golpistas[chave] = mensagem\n",
    "\n",
    "    with open(f'./results/topics/resultados_topics_atos_golpistas_{model}.txt', 'w') as arquivo:\n",
    "        for chave, mensagem in sumarios_atos_golpistas.items():\n",
    "            arquivo.write(f\"Tópico {chave}:\\n{mensagem}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2767ae3-5bfc-4d05-a928-a1192e12940b",
   "metadata": {},
   "source": [
    "### Base Lula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d24fd33-0db5-4b4d-ae86-92a727a22027",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelos = ['llama', 'mistral', 'bode', 'llama_3', 'llama_3.1']\n",
    "\n",
    "#Limpeza da VRAM\n",
    "device = cuda.get_current_device()\n",
    "device.reset()\n",
    "\n",
    "for model in modelos:\n",
    "    pipeline = None\n",
    "    llm = None\n",
    "    \n",
    "    if model not in ('llama_3', 'llama_3.1'):\n",
    "        try:\n",
    "            del llm\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        except:\n",
    "            continue\n",
    "        llm = define_model(model)\n",
    "    else:\n",
    "        try:\n",
    "            del pipeline\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        except:\n",
    "            continue\n",
    "        if model == 'llama_3':\n",
    "            model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "        elif model == 'llama_3.1':\n",
    "            model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "        pipeline = get_pipeline_llama3(model_id)\n",
    "\n",
    "    sumarios_lula = get_summaries(filter_top_twenty_topics_df_lula, model, llm, pipeline)\n",
    "    \n",
    "    for chave, mensagem in sumarios_lula.items():\n",
    "        if model not in ('llama_3', 'llama_3.1'):\n",
    "            mensagem = llama_translate(llm, mensagem)\n",
    "        else:\n",
    "            mensagem = llama_3_translate(pipeline, mensagem)\n",
    "        sumarios_lula[chave] = mensagem\n",
    "    \n",
    "    with open(f'./results/topics/resultados_topics_lula_{model}.txt', 'w') as arquivo:\n",
    "        for chave, mensagem in sumarios_lula.items():\n",
    "            arquivo.write(f\"Tópico {chave}:\\n{mensagem}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158b8ee3-9c13-4d37-b7d3-f406204279e9",
   "metadata": {},
   "source": [
    "### Base Bolsonaro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b63a5ca-00fe-4a1e-b64b-f31ec8dd6b96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelos = ['llama', 'mistral', 'bode', 'llama_3', 'llama_3.1']\n",
    "\n",
    "for model in modelos:\n",
    "    pipeline = None\n",
    "    llm = None\n",
    "    \n",
    "    if model not in ('llama_3', 'llama_3.1'):\n",
    "        try:\n",
    "            del llm\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        except:\n",
    "            continue\n",
    "        llm = define_model(model)\n",
    "    else:\n",
    "        try:\n",
    "            del pipeline\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        except:\n",
    "            continue\n",
    "        if model == 'llama_3':\n",
    "            model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "        elif model == 'llama_3.1':\n",
    "            model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "        pipeline = get_pipeline_llama3(model_id)\n",
    "\n",
    "    sumarios_bolsonaro = get_summaries(filter_top_twenty_topics_df_bolsonaro, model, llm, pipeline)\n",
    "    \n",
    "    for chave, mensagem in sumarios_bolsonaro.items():\n",
    "        if model not in ('llama_3', 'llama_3.1'):\n",
    "            mensagem = llama_translate(llm, mensagem)\n",
    "        else:\n",
    "            mensagem = llama_3_translate(pipeline, mensagem)\n",
    "        sumarios_bolsonaro[chave] = mensagem\n",
    "    \n",
    "    with open(f'./results/topics/resultados_topics_bolsonaro_{model}.txt', 'w') as arquivo:\n",
    "        for chave, mensagem in sumarios_bolsonaro.items():\n",
    "            arquivo.write(f\"Tópico {chave}:\\n{mensagem}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97bedc3-cac9-4985-8c8e-4bd5c5260e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del llm\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "except:\n",
    "    print(\"Variável não existe\")\n",
    "\n",
    "try:\n",
    "    del pipeline\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "except:\n",
    "    print(\"Variável não existe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e31d7d8-7bb2-430c-8ba9-53082514ee63",
   "metadata": {},
   "source": [
    "## Validações utilizando o BERTScore e BERTScore-p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3275dc8-35d8-4a6d-a580-319c42dca86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM, BertModel\n",
    "from bert_score import BERTScorer\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b59bc7e-acae-498c-be54-e2dcc32a643e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_summary_topics(caminho_arquivo):\n",
    "    try:\n",
    "        # Abrir o arquivo em modo de leitura com a codificação UTF-8\n",
    "        with open(caminho_arquivo, 'r', encoding='utf-8') as arquivo:\n",
    "            # Ler o conteúdo do arquivo\n",
    "            linhas = arquivo.readlines()\n",
    "\n",
    "            # Inicializar um dicionário vazio\n",
    "            dicionario = {}\n",
    "            chave_atual = None\n",
    "            valor_atual = \"\"\n",
    "\n",
    "            # Iterar sobre as linhas do arquivo\n",
    "            for linha in linhas:\n",
    "                # Remover espaços em branco extras no início e no final da linha\n",
    "                linha = linha.strip()\n",
    "                if linha.startswith(\"Tópico\"):\n",
    "                    # Se a linha começar com \"Tópico\", atualize a chave atual\n",
    "                    if chave_atual is not None:\n",
    "                        # Se já tiver uma chave atual, armazene o valor atual no dicionário\n",
    "                        dicionario[chave_atual] = valor_atual\n",
    "                    chave_atual = linha\n",
    "                    valor_atual = \"\"\n",
    "                else:\n",
    "                    # Se a linha não começar com \"Tópico\", adicione-a ao valor atual\n",
    "                    valor_atual += linha + \"\\n\"\n",
    "\n",
    "            # Armazene o último valor atual no dicionário\n",
    "            dicionario[chave_atual] = valor_atual\n",
    "\n",
    "        return dicionario\n",
    "    except FileNotFoundError:\n",
    "        print(\"Arquivo não encontrado.\")\n",
    "    except Exception as e:\n",
    "        print(\"Ocorreu um erro ao processar o arquivo:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30e16f3-4042-4e19-a6b8-1debe2e3d9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_para_string(df, topico):\n",
    "    # Filtrar o DataFrame pelo tópico especificado\n",
    "    df_filtrado = df[df['topics'] == int(topico)]\n",
    "    \n",
    "    # Concatenar os dados da coluna 'text' em uma única string\n",
    "    texto_completo = ' '.join(df_filtrado['text'])\n",
    "    \n",
    "    return texto_completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81b4823-ddd0-4862-a444-8c64c6f0273a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_bertscore_old(reference, candidate):\n",
    "    # BERTScore calculation\n",
    "    scorer = BERTScorer(model_type='microsoft/deberta-xlarge-mnli')\n",
    "    P, R, F1 = scorer.score([candidate], [reference])\n",
    "    \n",
    "    # Armazenar os resultados em um dicionário\n",
    "    result_dict = {\n",
    "        \"BERTScore Precision\": P.mean(),\n",
    "        \"BERTScore Recall\": R.mean(),\n",
    "        \"BERTScore F1\": F1.mean()\n",
    "    }\n",
    "\n",
    "    del scorer\n",
    "    gc.collect()\n",
    "    \n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6174ceb-e425-4b53-8590-f3e806f5b80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_bertscore(reference, candidate, segment_size=995, beta = 2):\n",
    "    from bert_score import BERTScorer\n",
    "    import gc\n",
    "    import torch\n",
    "    \n",
    "    def split_text(text, size):\n",
    "        \"\"\"\n",
    "        Divide o texto em segmentos de até 'size' caracteres,\n",
    "        garantindo que não corte palavras no meio.\n",
    "        \"\"\"\n",
    "        segments = []\n",
    "        start = 0\n",
    "        while start < len(text):\n",
    "            end = start + size\n",
    "            # Ajusta o final para o último espaço antes do limite\n",
    "            if end < len(text):\n",
    "                while end > start and text[end] not in [' ', '\\n', '.', ',']:\n",
    "                    end -= 1\n",
    "            if end == start:  # Caso não encontre delimitador válido\n",
    "                end = start + size\n",
    "            segments.append(text[start:end].strip())\n",
    "            start = end\n",
    "        return segments\n",
    "    \n",
    "    # Inicializa o scorer com o modelo desejado\n",
    "    scorer = BERTScorer(model_type='neuralmind/bert-large-portuguese-cased', num_layers=24)\n",
    "    \n",
    "    # Divide os textos em segmentos menores\n",
    "    reference_segments = split_text(reference, segment_size)\n",
    "    candidate_segments = split_text(candidate, segment_size)\n",
    "\n",
    "    weight = (len(candidate) ** beta) / ((len(candidate) ** beta) + len(reference))\n",
    "\n",
    "    P_list, R_list, F1_list = [], [], []\n",
    "\n",
    "    # Compara todos os segmentos do candidato com todos os da referência\n",
    "    for cand_seg in candidate_segments:\n",
    "        cand_len = len(cand_seg.split())\n",
    "        for ref_seg in reference_segments:\n",
    "            ref_len = len(ref_seg.split())\n",
    "            P, R, F1 = scorer.score([cand_seg], [ref_seg])\n",
    "            P_list.append(P.mean().item())\n",
    "            R_list.append(R.mean().item())\n",
    "            F1_list.append(F1.mean().item())\n",
    "\n",
    "    # Calcula a média dos Top-50 valores\n",
    "    def top_k_avg(scores, k=20):\n",
    "        if scores:\n",
    "            top_k_scores = sorted(scores, reverse=True)[:k]\n",
    "            return sum(top_k_scores) / len(top_k_scores)\n",
    "        return 0\n",
    "\n",
    "    # Calcula a média e o máximo dos resultados\n",
    "    result_dict = {\n",
    "        \"Weight\": weight,\n",
    "        \"Weighted F1 Avg\": ((sum(F1_list) / len(F1_list)) * weight) * 0.3 + (sum(F1_list) / len(F1_list)) * 0.7 if F1_list else 0,\n",
    "        \"Weighted F1 Max\": (max(F1_list) * weight) * 0.3 + max(F1_list) * 0.7 if F1_list else 0,\n",
    "        \"Weighted F1 Min\": (min(F1_list) * weight) * 0.3 + min(F1_list) * 0.7 if F1_list else 0,\n",
    "        \"Weighted F1 Top-k Avg\": (top_k_avg(F1_list) * weight) * 0.3 + top_k_avg(F1_list) * 0.7,\n",
    "        \"Precision Avg\": sum(P_list) / len(P_list) if P_list else 0,\n",
    "        \"Recall Avg\": sum(R_list) / len(R_list) if R_list else 0,\n",
    "        \"F1 Avg\": sum(F1_list) / len(F1_list) if F1_list else 0,\n",
    "        \"Precision Max\": max(P_list) if P_list else 0,\n",
    "        \"Recall Max\": max(R_list) if R_list else 0,\n",
    "        \"F1 Max\": max(F1_list) if F1_list else 0,\n",
    "        \"Precision Min\": min(P_list) if P_list else 0,\n",
    "        \"Recall Min\": min(R_list) if R_list else 0,\n",
    "        \"F1 Min\": min(F1_list) if F1_list else 0,\n",
    "        \"Precision Top-k Avg\": top_k_avg(P_list),\n",
    "        \"Recall Top-k Avg\": top_k_avg(R_list),\n",
    "        \"F1 Top-k Avg\": top_k_avg(F1_list),\n",
    "    }\n",
    "    \n",
    "    # Limpeza de memória\n",
    "    del scorer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451aafe3-ffa3-4965-a410-94d003b1422b",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_bolsonaro_llama = open_summary_topics('./results/topics/resultados_topics_bolsonaro_llama.txt')\n",
    "summary_lula_llama = open_summary_topics('./results/topics/resultados_topics_lula_llama.txt')\n",
    "summary_atos_golpistas_llama = open_summary_topics('./results/topics/resultados_topics_atos_golpistas_llama.txt')\n",
    "\n",
    "summary_bolsonaro_mistral = open_summary_topics('./results/topics/resultados_topics_bolsonaro_mistral.txt')\n",
    "summary_lula_mistral = open_summary_topics('./results/topics/resultados_topics_lula_mistral.txt')\n",
    "summary_atos_golpistas_mistral = open_summary_topics('./results/topics/resultados_topics_atos_golpistas_mistral.txt')\n",
    "\n",
    "summary_bolsonaro_bode = open_summary_topics('./results/topics/resultados_topics_bolsonaro_bode.txt')\n",
    "summary_lula_bode = open_summary_topics('./results/topics/resultados_topics_lula_bode.txt')\n",
    "summary_atos_golpistas_bode = open_summary_topics('./results/topics/resultados_topics_atos_golpistas_bode.txt')\n",
    "\n",
    "summary_bolsonaro_llama_3 = open_summary_topics('./results/topics/resultados_topics_bolsonaro_llama_3.txt')\n",
    "summary_lula_llama_3 = open_summary_topics('./results/topics/resultados_topics_lula_llama_3.txt')\n",
    "summary_atos_golpistas_llama_3 = open_summary_topics('./results/topics/resultados_topics_atos_golpistas_llama_3.txt')\n",
    "\n",
    "summary_bolsonaro_llama_3_1 = open_summary_topics('./results/topics/resultados_topics_bolsonaro_llama_3.1.txt')\n",
    "summary_lula_llama_3_1 = open_summary_topics('./results/topics/resultados_topics_lula_llama_3.1.txt')\n",
    "summary_atos_golpistas_llama_3_1 = open_summary_topics('./results/topics/resultados_topics_atos_golpistas_llama_3.1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f675b3e5-ccb9-43fc-acee-3ec99d81a298",
   "metadata": {},
   "outputs": [],
   "source": [
    "palavras_llama2 = []\n",
    "\n",
    "for i, topico in enumerate(summary_atos_golpistas_llama):\n",
    "    palavras = summary_atos_golpistas_llama[topico].split()\n",
    "    palavras_llama2.append(len(palavras))\n",
    "\n",
    "soma_llama2 = sum(palavras_llama2)\n",
    "tamanho_llama2 = len(palavras_llama2)\n",
    "\n",
    "media_llama2 = soma_llama2/tamanho_llama2\n",
    "\n",
    "print(\"Média Llama 2: \", media_llama2)\n",
    "\n",
    "\n",
    "for i, topico in enumerate(summary_atos_golpistas_llama_3):\n",
    "    palavras = summary_atos_golpistas_llama_3[topico].split()\n",
    "    palavras_llama2.append(len(palavras))\n",
    "\n",
    "soma_llama2 = sum(palavras_llama2)\n",
    "tamanho_llama2 = len(palavras_llama2)\n",
    "\n",
    "media_llama2 = soma_llama2/tamanho_llama2\n",
    "\n",
    "print(\"Média Llama 3: \", media_llama2)\n",
    "\n",
    "for i, topico in enumerate(summary_atos_golpistas_mistral):\n",
    "    palavras = summary_atos_golpistas_mistral[topico].split()\n",
    "    palavras_llama2.append(len(palavras))\n",
    "\n",
    "soma_llama2 = sum(palavras_llama2)\n",
    "tamanho_llama2 = len(palavras_llama2)\n",
    "\n",
    "media_llama2 = soma_llama2/tamanho_llama2\n",
    "\n",
    "print(\"Média Mistral: \", media_llama2)\n",
    "\n",
    "for i, topico in enumerate(summary_atos_golpistas_bode):\n",
    "    palavras = summary_atos_golpistas_bode[topico].split()\n",
    "    palavras_llama2.append(len(palavras))\n",
    "\n",
    "soma_llama2 = sum(palavras_llama2)\n",
    "tamanho_llama2 = len(palavras_llama2)\n",
    "\n",
    "media_llama2 = soma_llama2/tamanho_llama2\n",
    "\n",
    "print(\"Média Bode: \", media_llama2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e3c338-649c-47e5-8c0f-037810696cc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### result_bolsonaro = {}\n",
    "\n",
    "print(\"Sumários Bolsonaro\")\n",
    "\n",
    "print(\"BERTSCore Llama\")\n",
    "for chave, valor in summary_bolsonaro_llama.items():\n",
    "    numero_topico = chave.split()[1].replace(\":\", \"\")\n",
    "    print(\"Tópico:\", numero_topico)\n",
    "\n",
    "    texto_completo = dataframe_para_string(filter_top_twenty_topics_df_bolsonaro, numero_topico)\n",
    "    result = validate_bertscore(texto_completo, valor)\n",
    "\n",
    "    result_bolsonaro[f'BertScore Bolsonaro - Llama 2 - Tópico {numero_topico}: '] = result\n",
    "\n",
    "print(\"BERTSCore Mistral\")\n",
    "for chave, valor in summary_bolsonaro_mistral.items():\n",
    "    numero_topico = chave.split()[1].replace(\":\", \"\")\n",
    "    print(\"Tópico:\", numero_topico)\n",
    "\n",
    "    texto_completo = dataframe_para_string(filter_top_twenty_topics_df_bolsonaro, numero_topico)\n",
    "    result = validate_bertscore(texto_completo, valor)\n",
    "\n",
    "    result_bolsonaro[f'BertScore Bolsonaro - Mistral - Tópico {numero_topico}: '] = result\n",
    "\n",
    "print(\"BERTSCore Bode\")\n",
    "for chave, valor in summary_bolsonaro_bode.items():\n",
    "    numero_topico = chave.split()[1].replace(\":\", \"\")\n",
    "    print(\"Tópico:\", numero_topico)\n",
    "\n",
    "    texto_completo = dataframe_para_string(filter_top_twenty_topics_df_bolsonaro, numero_topico)\n",
    "    result = validate_bertscore(texto_completo, valor)\n",
    "\n",
    "    result_bolsonaro[f'BertScore Bolsonaro - Bode - Tópico {numero_topico}: '] = result\n",
    "\n",
    "print(\"BERTSCore Llama 3\")\n",
    "for chave, valor in summary_bolsonaro_llama_3.items():\n",
    "    numero_topico = chave.split()[1].replace(\":\", \"\")\n",
    "    print(\"Tópico:\", numero_topico)\n",
    "\n",
    "    texto_completo = dataframe_para_string(filter_top_twenty_topics_df_bolsonaro, numero_topico)\n",
    "    result = validate_bertscore(texto_completo, valor)\n",
    "\n",
    "    result_bolsonaro[f'BertScore Bolsonaro - Llama 3 - Tópico {numero_topico}: '] = result\n",
    "\n",
    "print(\"BERTSCore Llama 3.1\")\n",
    "for chave, valor in summary_bolsonaro_llama_3_1.items():\n",
    "    numero_topico = chave.split()[1].replace(\":\", \"\")\n",
    "    print(\"Tópico:\", numero_topico)\n",
    "\n",
    "    texto_completo = dataframe_para_string(filter_top_twenty_topics_df_bolsonaro, numero_topico)\n",
    "    result = validate_bertscore(texto_completo, valor)\n",
    "\n",
    "    result_bolsonaro[f'BertScore Bolsonaro - Llama 3.1 - Tópico {numero_topico}: '] = result\n",
    "\n",
    "with open('./results/topics/result_bertscore_bolsonaro_v6.txt', 'w') as file:\n",
    "    for key, value in result_bolsonaro.items():\n",
    "        file.write(f\"{key}:\\n\")\n",
    "        for sub_key, sub_value in value.items():\n",
    "            file.write(f\"\\t{sub_key}: {sub_value}\\n\")\n",
    "        file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635fb7c3-3d82-43fa-b789-4b7fe0e24538",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Sumários Lula\")\n",
    "\n",
    "result_lula = {}\n",
    "\n",
    "print(\"BERTSCore Llama\")\n",
    "for chave, valor in summary_lula_llama.items():\n",
    "    numero_topico = chave.split()[1].replace(\":\", \"\")\n",
    "    print(\"Tópico:\", numero_topico)\n",
    "\n",
    "    texto_completo = dataframe_para_string(filter_top_twenty_topics_df_lula, numero_topico)\n",
    "    result = validate_bertscore(texto_completo, valor)\n",
    "\n",
    "    result_lula[f'BertScore Lula - Llama 2 - Tópico {numero_topico}: '] = result\n",
    "\n",
    "print(\"BERTSCore Mistral\")\n",
    "for chave, valor in summary_lula_mistral.items():\n",
    "    numero_topico = chave.split()[1].replace(\":\", \"\")\n",
    "    print(\"Tópico:\", numero_topico)\n",
    "\n",
    "    texto_completo = dataframe_para_string(filter_top_twenty_topics_df_lula, numero_topico)\n",
    "    result = validate_bertscore(texto_completo, valor)\n",
    "\n",
    "    result_lula[f'BertScore Lula - Mistral - Tópico {numero_topico}: '] = result\n",
    "\n",
    "print(\"BERTSCore Bode\")\n",
    "for chave, valor in summary_lula_bode.items():\n",
    "    numero_topico = chave.split()[1].replace(\":\", \"\")\n",
    "    print(\"Tópico:\", numero_topico)\n",
    "\n",
    "    texto_completo = dataframe_para_string(filter_top_twenty_topics_df_lula, numero_topico)\n",
    "    result = validate_bertscore(texto_completo, valor)\n",
    "\n",
    "    result_lula[f'BertScore Lula - Bode - Tópico {numero_topico}: '] = result\n",
    "\n",
    "print(\"BERTSCore Llama 3\")\n",
    "for chave, valor in summary_lula_llama_3.items():\n",
    "    numero_topico = chave.split()[1].replace(\":\", \"\")\n",
    "    print(\"Tópico:\", numero_topico)\n",
    "\n",
    "    texto_completo = dataframe_para_string(filter_top_twenty_topics_df_lula, numero_topico)\n",
    "    result = validate_bertscore(texto_completo, valor)\n",
    "\n",
    "    result_lula[f'BertScore Lula - Llama 3 - Tópico {numero_topico}: '] = result\n",
    "\n",
    "print(\"BERTSCore Llama 3.1\")\n",
    "for chave, valor in summary_lula_llama_3_1.items():\n",
    "    numero_topico = chave.split()[1].replace(\":\", \"\")\n",
    "    print(\"Tópico:\", numero_topico)\n",
    "\n",
    "    texto_completo = dataframe_para_string(filter_top_twenty_topics_df_lula, numero_topico)\n",
    "    result = validate_bertscore(texto_completo, valor)\n",
    "\n",
    "    result_lula[f'BertScore Lula - Llama 3.1 - Tópico {numero_topico}: '] = result\n",
    "\n",
    "\n",
    "with open('./results/topics/result_bertscore_lula_v6.txt', 'w') as file:\n",
    "    for key, value in result_lula.items():\n",
    "        file.write(f\"{key}:\\n\")\n",
    "        for sub_key, sub_value in value.items():\n",
    "            file.write(f\"\\t{sub_key}: {sub_value}\\n\")\n",
    "        file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699e221a-5c2c-4e54-bacc-44a1aeb2271d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sumários Atos Golpistas\")\n",
    "\n",
    "result_atos_golpistas = {}\n",
    "\n",
    "print(\"BERTSCore Llama\")\n",
    "for chave, valor in summary_atos_golpistas_llama.items():\n",
    "    numero_topico = chave.split()[1].replace(\":\", \"\")\n",
    "    print(\"Tópico:\", numero_topico)\n",
    "\n",
    "    texto_completo = dataframe_para_string(filter_top_twenty_topics_df_atos_golpistas, numero_topico)\n",
    "    result = validate_bertscore(texto_completo, valor)\n",
    "\n",
    "    result_atos_golpistas[f'BertScore Atos Golpistas - Llama 2 - Tópico {numero_topico}: '] = result\n",
    "\n",
    "print(\"BERTSCore Mistral\")\n",
    "for chave, valor in summary_atos_golpistas_mistral.items():\n",
    "    numero_topico = chave.split()[1].replace(\":\", \"\")\n",
    "    print(\"Tópico:\", numero_topico)\n",
    "\n",
    "    texto_completo = dataframe_para_string(filter_top_twenty_topics_df_atos_golpistas, numero_topico)\n",
    "\n",
    "    result = validate_bertscore(texto_completo, valor)\n",
    "\n",
    "    result_atos_golpistas[f'BertScore Atos Golpistas - Mistral - Tópico {numero_topico}: '] = result\n",
    "\n",
    "print(\"BERTSCore Bode\")\n",
    "for chave, valor in summary_atos_golpistas_bode.items():\n",
    "    numero_topico = chave.split()[1].replace(\":\", \"\")\n",
    "    print(\"Tópico:\", numero_topico)\n",
    "\n",
    "    texto_completo = dataframe_para_string(filter_top_twenty_topics_df_atos_golpistas, numero_topico)\n",
    "\n",
    "    result = validate_bertscore(texto_completo, valor)\n",
    "\n",
    "    result_atos_golpistas[f'BertScore Atos Golpistas - Bode - Tópico {numero_topico}: '] = result\n",
    "\n",
    "print(\"BERTSCore Llama 3\")\n",
    "for chave, valor in summary_atos_golpistas_llama_3.items():\n",
    "    numero_topico = chave.split()[1].replace(\":\", \"\")\n",
    "    print(\"Tópico:\", numero_topico)\n",
    "\n",
    "    texto_completo = dataframe_para_string(filter_top_twenty_topics_df_atos_golpistas, numero_topico)\n",
    "    result = validate_bertscore(texto_completo, valor)\n",
    "\n",
    "    result_atos_golpistas[f'BertScore Atos Golpistas - Llama 3 - Tópico {numero_topico}: '] = result\n",
    "\n",
    "print(\"BERTSCore Llama 3.1\")\n",
    "for chave, valor in summary_atos_golpistas_llama_3_1.items():\n",
    "    numero_topico = chave.split()[1].replace(\":\", \"\")\n",
    "    print(\"Tópico:\", numero_topico)\n",
    "\n",
    "    texto_completo = dataframe_para_string(filter_top_twenty_topics_df_atos_golpistas, numero_topico)\n",
    "    result = validate_bertscore(texto_completo, valor)\n",
    "\n",
    "    result_atos_golpistas[f'BertScore Atos Golpistas - Llama 3.1 - Tópico {numero_topico}: '] = result\n",
    "\n",
    "with open('./results/topics/result_bertscore_atos_golpistas_v6.txt', 'w') as file:\n",
    "    for key, value in result_atos_golpistas.items():\n",
    "        file.write(f\"{key}:\\n\")\n",
    "        for sub_key, sub_value in value.items():\n",
    "            file.write(f\"\\t{sub_key}: {sub_value}\\n\")\n",
    "        file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30615212-387f-4cf4-beb2-b316859f7127",
   "metadata": {},
   "source": [
    "## Validações utilizando o Rouge Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129e4bfa-8956-4d22-b66c-a34a1dd8177a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score.rouge_scorer import RougeScorer\n",
    "scorer = RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49f48b3-0c63-4a4d-8396-5fbdc4ee4d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_bolsonaro_rouge = {}\n",
    "\n",
    "print(\"Sumários Bolsonaro\")\n",
    "\n",
    "print(\"BERTSCore Llama\")\n",
    "for chave, valor in summary_bolsonaro_llama.items():\n",
    "    numero_topico = chave.split()[1].replace(\":\", \"\")\n",
    "    print(\"Tópico:\", numero_topico)\n",
    "\n",
    "    texto_completo = dataframe_para_string(filter_top_twenty_topics_df_bolsonaro, numero_topico)\n",
    "    result = scorer.score(texto_completo, valor).items()\n",
    "\n",
    "    result_bolsonaro_rouge[f'Rouge Score - Bolsonaro - Llama 2 - Tópico {numero_topico}: '] = result\n",
    "\n",
    "print(\"BERTSCore Mistral\")\n",
    "for chave, valor in summary_bolsonaro_mistral.items():\n",
    "    numero_topico = chave.split()[1].replace(\":\", \"\")\n",
    "    print(\"Tópico:\", numero_topico)\n",
    "\n",
    "    texto_completo = dataframe_para_string(filter_top_twenty_topics_df_bolsonaro, numero_topico)\n",
    "    result = scorer.score(texto_completo, valor).items()\n",
    "\n",
    "    result_bolsonaro_rouge[f'Rouge Score - Bolsonaro - Mistral - Tópico {numero_topico}: '] = result\n",
    "\n",
    "print(\"BERTSCore Bode\")\n",
    "for chave, valor in summary_bolsonaro_bode.items():\n",
    "    numero_topico = chave.split()[1].replace(\":\", \"\")\n",
    "    print(\"Tópico:\", numero_topico)\n",
    "\n",
    "    texto_completo = dataframe_para_string(filter_top_twenty_topics_df_bolsonaro, numero_topico)\n",
    "    result = scorer.score(texto_completo, valor).items()\n",
    "\n",
    "    result_bolsonaro_rouge[f'Rouge Score - Bolsonaro - Bode - Tópico {numero_topico}: '] = result\n",
    "\n",
    "print(\"BERTSCore Llama 3\")\n",
    "for chave, valor in summary_bolsonaro_llama_3.items():\n",
    "    numero_topico = chave.split()[1].replace(\":\", \"\")\n",
    "    print(\"Tópico:\", numero_topico)\n",
    "\n",
    "    texto_completo = dataframe_para_string(filter_top_twenty_topics_df_bolsonaro, numero_topico)\n",
    "    result = scorer.score(texto_completo, valor).items()\n",
    "\n",
    "    result_bolsonaro_rouge[f'Rouge Score - Bolsonaro - Llama 3 - Tópico {numero_topico}: '] = result\n",
    "\n",
    "print(\"BERTSCore Llama 3.1\")\n",
    "for chave, valor in summary_bolsonaro_llama_3_1.items():\n",
    "    numero_topico = chave.split()[1].replace(\":\", \"\")\n",
    "    print(\"Tópico:\", numero_topico)\n",
    "\n",
    "    texto_completo = dataframe_para_string(filter_top_twenty_topics_df_bolsonaro, numero_topico)\n",
    "    result = scorer.score(texto_completo, valor).items()\n",
    "\n",
    "    result_bolsonaro_rouge[f'Rouge Score - Bolsonaro - Llama 3.1 - Tópico {numero_topico}: '] = result\n",
    "\n",
    "with open('./results/topics/result_bertscore_bolsonaro_rougescore.txt', 'w') as file:\n",
    "    for key, value in result_bolsonaro_rouge.items():\n",
    "        file.write(f\"\\t{key} {value}\\n\")\n",
    "        file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f681055-37f7-4a62-bef5-5c2bc248eab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sumários Lula\")\n",
    "\n",
    "result_lula_rouge = {}\n",
    "\n",
    "print(\"BERTSCore Llama\")\n",
    "for chave, valor in summary_lula_llama.items():\n",
    "    numero_topico = chave.split()[1].replace(\":\", \"\")\n",
    "    print(\"Tópico:\", numero_topico)\n",
    "\n",
    "    texto_completo = dataframe_para_string(filter_top_twenty_topics_df_lula, numero_topico)\n",
    "    result = scorer.score(texto_completo, valor).items()\n",
    "\n",
    "    result_lula_rouge[f'Rouge Score - Lula - Llama 2 - Tópico {numero_topico}: '] = result\n",
    "\n",
    "print(\"BERTSCore Mistral\")\n",
    "for chave, valor in summary_lula_mistral.items():\n",
    "    numero_topico = chave.split()[1].replace(\":\", \"\")\n",
    "    print(\"Tópico:\", numero_topico)\n",
    "\n",
    "    texto_completo = dataframe_para_string(filter_top_twenty_topics_df_lula, numero_topico)\n",
    "    result = scorer.score(texto_completo, valor).items()\n",
    "\n",
    "    result_lula_rouge[f'Rouge Score - Lula - Mistral - Tópico {numero_topico}: '] = result\n",
    "\n",
    "print(\"BERTSCore Bode\")\n",
    "for chave, valor in summary_lula_bode.items():\n",
    "    numero_topico = chave.split()[1].replace(\":\", \"\")\n",
    "    print(\"Tópico:\", numero_topico)\n",
    "\n",
    "    texto_completo = dataframe_para_string(filter_top_twenty_topics_df_lula, numero_topico)\n",
    "    result = scorer.score(texto_completo, valor).items()\n",
    "\n",
    "    result_lula_rouge[f'Rouge Score - Lula - Bode - Tópico {numero_topico}: '] = result\n",
    "\n",
    "print(\"BERTSCore Llama 3\")\n",
    "for chave, valor in summary_lula_llama_3.items():\n",
    "    numero_topico = chave.split()[1].replace(\":\", \"\")\n",
    "    print(\"Tópico:\", numero_topico)\n",
    "\n",
    "    texto_completo = dataframe_para_string(filter_top_twenty_topics_df_lula, numero_topico)\n",
    "    result = scorer.score(texto_completo, valor).items()\n",
    "\n",
    "    result_lula_rouge[f'Rouge Score - Lula - Llama 3 - Tópico {numero_topico}: '] = result\n",
    "\n",
    "print(\"BERTSCore Llama 3.1\")\n",
    "for chave, valor in summary_lula_llama_3_1.items():\n",
    "    numero_topico = chave.split()[1].replace(\":\", \"\")\n",
    "    print(\"Tópico:\", numero_topico)\n",
    "\n",
    "    texto_completo = dataframe_para_string(filter_top_twenty_topics_df_lula, numero_topico)\n",
    "    result = scorer.score(texto_completo, valor).items()\n",
    "\n",
    "    result_lula_rouge[f'Rouge Score - Lula - Llama 3.1 - Tópico {numero_topico}: '] = result\n",
    "\n",
    "\n",
    "with open('./results/topics/result_bertscore_lula_rougescore.txt', 'w') as file:\n",
    "    for key, value in result_lula_rouge.items():\n",
    "        file.write(f\"\\t{key} {value}\\n\")\n",
    "        file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa569e97-bf9e-40e2-b445-2466f15f2e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sumários Atos Golpistas\")\n",
    "\n",
    "result_atos_golpistas_rouge = {}\n",
    "\n",
    "print(\"BERTSCore Llama\")\n",
    "for chave, valor in summary_atos_golpistas_llama.items():\n",
    "    numero_topico = chave.split()[1].replace(\":\", \"\")\n",
    "    print(\"Tópico:\", numero_topico)\n",
    "\n",
    "    texto_completo = dataframe_para_string(filter_top_twenty_topics_df_atos_golpistas, numero_topico)\n",
    "    result = validate_bertscore(texto_completo, valor)\n",
    "\n",
    "    result_atos_golpistas_rouge[f'Rouge Score - Atos Golpistas - Llama 2 - Tópico {numero_topico}: '] = result\n",
    "\n",
    "print(\"BERTSCore Mistral\")\n",
    "for chave, valor in summary_atos_golpistas_mistral.items():\n",
    "    numero_topico = chave.split()[1].replace(\":\", \"\")\n",
    "    print(\"Tópico:\", numero_topico)\n",
    "\n",
    "    texto_completo = dataframe_para_string(filter_top_twenty_topics_df_atos_golpistas, numero_topico)\n",
    "\n",
    "    result = validate_bertscore(texto_completo, valor)\n",
    "\n",
    "    result_atos_golpistas_rouge[f'Rouge Score - Atos Golpistas - Mistral - Tópico {numero_topico}: '] = result\n",
    "\n",
    "print(\"BERTSCore Bode\")\n",
    "for chave, valor in summary_atos_golpistas_bode.items():\n",
    "    numero_topico = chave.split()[1].replace(\":\", \"\")\n",
    "    print(\"Tópico:\", numero_topico)\n",
    "\n",
    "    texto_completo = dataframe_para_string(filter_top_twenty_topics_df_atos_golpistas, numero_topico)\n",
    "\n",
    "    result = validate_bertscore(texto_completo, valor)\n",
    "\n",
    "    result_atos_golpistas_rouge[f'Rouge Score - Atos Golpistas - Bode - Tópico {numero_topico}: '] = result\n",
    "\n",
    "print(\"BERTSCore Llama 3\")\n",
    "for chave, valor in summary_atos_golpistas_llama_3.items():\n",
    "    numero_topico = chave.split()[1].replace(\":\", \"\")\n",
    "    print(\"Tópico:\", numero_topico)\n",
    "\n",
    "    texto_completo = dataframe_para_string(filter_top_twenty_topics_df_atos_golpistas, numero_topico)\n",
    "    result = validate_bertscore(texto_completo, valor)\n",
    "\n",
    "    result_atos_golpistas_rouge[f'Rouge Score - Atos Golpistas - Llama 3 - Tópico {numero_topico}: '] = result\n",
    "\n",
    "print(\"BERTSCore Llama 3.1\")\n",
    "for chave, valor in summary_atos_golpistas_llama_3_1.items():\n",
    "    numero_topico = chave.split()[1].replace(\":\", \"\")\n",
    "    print(\"Tópico:\", numero_topico)\n",
    "\n",
    "    texto_completo = dataframe_para_string(filter_top_twenty_topics_df_atos_golpistas, numero_topico)\n",
    "    result = validate_bertscore(texto_completo, valor)\n",
    "\n",
    "    result_atos_golpistas_rouge[f'Rouge Score - Atos Golpistas - Llama 3.1 - Tópico {numero_topico}: '] = result\n",
    "\n",
    "with open('./results/topics/result_bertscore_atos_golpistas_rougescore.txt', 'w') as file:\n",
    "    for key, value in result_atos_golpistas_rouge.items():\n",
    "        file.write(f\"\\t{key} {value}\\n\")\n",
    "        file.write(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
